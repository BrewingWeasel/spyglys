---
source: src/parser.rs
expression: "Lexer::new(\"let\n\n    def   \n\n            end   let   def end defendletdef\").tokenize()"
---
[
    Spanned {
        start: 0,
        end: 3,
        contents: Let,
    },
    Spanned {
        start: 9,
        end: 12,
        contents: Def,
    },
    Spanned {
        start: 29,
        end: 32,
        contents: Ident(
            "end",
        ),
    },
    Spanned {
        start: 35,
        end: 38,
        contents: Let,
    },
    Spanned {
        start: 41,
        end: 44,
        contents: Def,
    },
    Spanned {
        start: 45,
        end: 48,
        contents: Ident(
            "end",
        ),
    },
    Spanned {
        start: 49,
        end: 61,
        contents: Ident(
            "defendletdef",
        ),
    },
    Spanned {
        start: 61,
        end: 61,
        contents: Eof,
    },
]
